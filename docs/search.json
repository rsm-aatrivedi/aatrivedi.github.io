[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Welcome to my blog. Here are some of my projects!\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nAashvi\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Clusters to Classifications: K-Means and KNN in R\n\n\n\n\n\n\nAashvi\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nAashvi\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nAashvi\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Blog/Project2/index.html",
    "href": "Blog/Project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Read the dataset\nblueprinty &lt;- read_csv(\"/home/jovyan/Downloads/SPRING/Marketing Analytics/MA_demo/mysite/Blog/Project2/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View first few rows\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n# Histogram of patents by iscustomer\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\ngeom_histogram(binwidth = 1, alpha = 0.6, position = \"identity\") +\nscale_fill_manual(values = c(\"steelblue\", \"seagreen\"),\n                  labels = c(\"Non-customer\", \"Customer\")) +\nlabs(title = \"Distribution of Patents by Customer Status\",\n      x = \"Number of Patents\", y = \"Count\", fill = \"Customer Status\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean patents by customer status\nblueprinty %&gt;%\ngroup_by(iscustomer) %&gt;%\nsummarise(mean_patents = mean(patents), n = n())\n\n# A tibble: 2 × 3\n  iscustomer mean_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n1          0         3.47  1019\n2          1         4.13   481\n\n\nThe histogram and summary table show that Blueprinty customers tend to have more patents on average than non-customers. However, this raw difference could be driven by other variables like firm age or region, so we should not infer causality yet.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution by iscustomer\nblueprinty %&gt;%\ngroup_by(region, iscustomer) %&gt;%\nsummarise(count = n(), .groups = \"drop\") %&gt;%\ntidyr::pivot_wider(names_from = iscustomer, values_from = count, values_fill = 0) %&gt;%\nrename(`Non-customer` = `0`, `Customer` = `1`)\n\n# A tibble: 5 × 3\n  region    `Non-customer` Customer\n  &lt;chr&gt;              &lt;int&gt;    &lt;int&gt;\n1 Midwest              187       37\n2 Northeast            273      328\n3 Northwest            158       29\n4 South                156       35\n5 Southwest            245       52\n\n\n\n# Histogram of age by iscustomer\nggplot(blueprinty, aes(x = age, fill = factor(iscustomer))) +\ngeom_histogram(binwidth = 2.5, alpha = 0.6, position = \"identity\") +\nscale_fill_manual(values = c(\"orange\", \"darkgreen\"),\n                  labels = c(\"Non-customer\", \"Customer\")) +\nlabs(title = \"Distribution of Firm Age by Customer Status\",\n      x = \"Firm Age (Years)\", y = \"Count\", fill = \"Customer Status\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean age by customer status\nblueprinty %&gt;%\ngroup_by(iscustomer) %&gt;%\nsummarise(mean_age = mean(age))\n\n# A tibble: 2 × 2\n  iscustomer mean_age\n       &lt;dbl&gt;    &lt;dbl&gt;\n1          0     26.1\n2          1     26.9\n\n\nCustomers tend to be older firms and are distributed differently across regions compared to non-customers. This supports the idea that Blueprinty customers are not randomly selected, and it will be important to control for region and age in further analysis.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n# Use number of patents as outcome Y\nY &lt;- blueprinty$patents\n\n\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe likelihood of observing a sample of \\(n\\) independent values is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood is: \\[\n\\log L(\\lambda) = -n\\lambda + (\\log \\lambda) \\sum Y_i - \\sum \\log(Y_i!)\n\\]\n\n\n# Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\nif (lambda &lt;= 0) return(-Inf)\nn &lt;- length(Y)\n  loglik &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\nreturn(loglik)\n}\n\n\n# Try values of lambda from 0.1 to 10\nlambda_vals &lt;- seq(0.1, 10, length.out = 200)\nloglik_vals &lt;- sapply(lambda_vals, poisson_loglikelihood, Y = Y)\n\n# Plot\nplot(lambda_vals, loglik_vals, type = \"l\", col = \"darkblue\", lwd = 2,\nmain = \"Poisson Log-Likelihood as Function of λ\",\nxlab = \"Lambda\", ylab = \"Log-Likelihood\")\nabline(v = mean(Y), col = \"red\", lty = 2)  # Add MLE line at mean(Y)\n\n\n\n\n\n\n\n\nThe log-likelihood peaks around the sample mean of Y, which visually confirms that \\(\\lambda_{MLE} = \\bar{Y}\\).\n\nmean(Y)\n\n[1] 3.684667\n\n\nIf we take the derivative of the log-likelihood and set it to 0, we find: &gt; \\[\n\\frac{\\partial \\log L}{\\partial \\lambda} = -n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\Rightarrow \\hat{\\lambda} = \\bar{Y}\n\\]\n&gt; The sample mean is the MLE of \\(\\lambda\\), which makes intuitive sense because the Poisson distribution has mean = \\(\\lambda\\).\n\n# Negative log-likelihood (for minimization)\nneg_loglik &lt;- function(lambda, Y) {\nreturn(-poisson_loglikelihood(lambda, Y))\n}\n\n# Optimize\noptim_result &lt;- optim(par = 1, fn = neg_loglik, Y = Y, method = \"Brent\", lower = 0.01, upper = 20)\noptim_result\n\n$par\n[1] 3.684667\n\n$value\n[1] 3367.684\n\n$counts\nfunction gradient \n      NA       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optimizer confirms that the MLE of \\(\\lambda\\) is approximately equal to the sample mean. This gives us confidence that our custom likelihood function and MLE implementation are correct.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Create squared age and dummy variables for region\nblueprinty &lt;- blueprinty %&gt;%\nmutate(age2 = age^2)\n\n# Construct the covariate matrix X (intercept, age, age^2, region dummies, iscustomer)\nX &lt;- model.matrix(~ age + age2 + region + iscustomer, data = blueprinty)\n\n# Outcome variable\nY &lt;- blueprinty$patents\n\n\n\n\n# Define log-likelihood function\npoisson_regression_loglik &lt;- function(beta, Y, X) {\nlambda &lt;- exp(X %*% beta)\n  loglik &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\nreturn(-loglik)  # negative log-likelihood for minimization\n}\n\n\n\n\n\n# Initial guess: all betas = 0\ninit_beta &lt;- rep(0, ncol(X))\n\n# Estimate MLE via optim()\nfit &lt;- optim(par = init_beta,\n            fn = poisson_regression_loglik,\n            Y = Y, X = X,\n            hessian = TRUE, method = \"BFGS\")\n\n# Extract coefficients\nbeta_hat &lt;- fit$par\n\n# Calculate standard errors from Hessian\nhessian &lt;- fit$hessian\nvcov_matrix &lt;- solve(hessian)\nse_beta &lt;- sqrt(diag(vcov_matrix))\n\n# Create table\ncoef_table &lt;- data.frame(\nCoefficient = beta_hat,\nStd_Error = se_beta,\nrow.names = colnames(X)\n)\n\nknitr::kable(coef_table, digits = 4, caption = \"MLE Coefficients and Standard Errors\")\n\n\nMLE Coefficients and Standard Errors\n\n\n\nCoefficient\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage2\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\n\n\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n            data = blueprinty, family = poisson())\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe coefficients from both optim() and glm() are nearly identical, indicating correct implementation.\nThe coefficient on iscustomer is positive and statistically significant, suggesting that firms using Blueprinty software tend to be granted more patents, controlling for other variables.\n\n\n\n\n# Counterfactual prediction: all firms as non-customers (iscustomer = 0)\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\n\n# Counterfactual prediction: all firms as customers (iscustomer = 1)\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n\n# Predicted patent counts under each case\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Average treatment effect\ntreatment_effect &lt;- mean(lambda_1 - lambda_0)\ntreatment_effect\n\n[1] 0.2178843\n\n\nOn average, being a Blueprinty customer increases the expected number of patents by about 0.22 over five years per firm."
  },
  {
    "objectID": "Blog/Project2/index.html#blueprinty-case-study",
    "href": "Blog/Project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Read the dataset\nblueprinty &lt;- read_csv(\"/home/jovyan/Downloads/SPRING/Marketing Analytics/MA_demo/mysite/Blog/Project2/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View first few rows\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n# Histogram of patents by iscustomer\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\ngeom_histogram(binwidth = 1, alpha = 0.6, position = \"identity\") +\nscale_fill_manual(values = c(\"steelblue\", \"seagreen\"),\n                  labels = c(\"Non-customer\", \"Customer\")) +\nlabs(title = \"Distribution of Patents by Customer Status\",\n      x = \"Number of Patents\", y = \"Count\", fill = \"Customer Status\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean patents by customer status\nblueprinty %&gt;%\ngroup_by(iscustomer) %&gt;%\nsummarise(mean_patents = mean(patents), n = n())\n\n# A tibble: 2 × 3\n  iscustomer mean_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n1          0         3.47  1019\n2          1         4.13   481\n\n\nThe histogram and summary table show that Blueprinty customers tend to have more patents on average than non-customers. However, this raw difference could be driven by other variables like firm age or region, so we should not infer causality yet.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution by iscustomer\nblueprinty %&gt;%\ngroup_by(region, iscustomer) %&gt;%\nsummarise(count = n(), .groups = \"drop\") %&gt;%\ntidyr::pivot_wider(names_from = iscustomer, values_from = count, values_fill = 0) %&gt;%\nrename(`Non-customer` = `0`, `Customer` = `1`)\n\n# A tibble: 5 × 3\n  region    `Non-customer` Customer\n  &lt;chr&gt;              &lt;int&gt;    &lt;int&gt;\n1 Midwest              187       37\n2 Northeast            273      328\n3 Northwest            158       29\n4 South                156       35\n5 Southwest            245       52\n\n\n\n# Histogram of age by iscustomer\nggplot(blueprinty, aes(x = age, fill = factor(iscustomer))) +\ngeom_histogram(binwidth = 2.5, alpha = 0.6, position = \"identity\") +\nscale_fill_manual(values = c(\"orange\", \"darkgreen\"),\n                  labels = c(\"Non-customer\", \"Customer\")) +\nlabs(title = \"Distribution of Firm Age by Customer Status\",\n      x = \"Firm Age (Years)\", y = \"Count\", fill = \"Customer Status\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean age by customer status\nblueprinty %&gt;%\ngroup_by(iscustomer) %&gt;%\nsummarise(mean_age = mean(age))\n\n# A tibble: 2 × 2\n  iscustomer mean_age\n       &lt;dbl&gt;    &lt;dbl&gt;\n1          0     26.1\n2          1     26.9\n\n\nCustomers tend to be older firms and are distributed differently across regions compared to non-customers. This supports the idea that Blueprinty customers are not randomly selected, and it will be important to control for region and age in further analysis.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n# Use number of patents as outcome Y\nY &lt;- blueprinty$patents\n\n\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe likelihood of observing a sample of \\(n\\) independent values is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood is: \\[\n\\log L(\\lambda) = -n\\lambda + (\\log \\lambda) \\sum Y_i - \\sum \\log(Y_i!)\n\\]\n\n\n# Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\nif (lambda &lt;= 0) return(-Inf)\nn &lt;- length(Y)\n  loglik &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\nreturn(loglik)\n}\n\n\n# Try values of lambda from 0.1 to 10\nlambda_vals &lt;- seq(0.1, 10, length.out = 200)\nloglik_vals &lt;- sapply(lambda_vals, poisson_loglikelihood, Y = Y)\n\n# Plot\nplot(lambda_vals, loglik_vals, type = \"l\", col = \"darkblue\", lwd = 2,\nmain = \"Poisson Log-Likelihood as Function of λ\",\nxlab = \"Lambda\", ylab = \"Log-Likelihood\")\nabline(v = mean(Y), col = \"red\", lty = 2)  # Add MLE line at mean(Y)\n\n\n\n\n\n\n\n\nThe log-likelihood peaks around the sample mean of Y, which visually confirms that \\(\\lambda_{MLE} = \\bar{Y}\\).\n\nmean(Y)\n\n[1] 3.684667\n\n\nIf we take the derivative of the log-likelihood and set it to 0, we find: &gt; \\[\n\\frac{\\partial \\log L}{\\partial \\lambda} = -n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\Rightarrow \\hat{\\lambda} = \\bar{Y}\n\\]\n&gt; The sample mean is the MLE of \\(\\lambda\\), which makes intuitive sense because the Poisson distribution has mean = \\(\\lambda\\).\n\n# Negative log-likelihood (for minimization)\nneg_loglik &lt;- function(lambda, Y) {\nreturn(-poisson_loglikelihood(lambda, Y))\n}\n\n# Optimize\noptim_result &lt;- optim(par = 1, fn = neg_loglik, Y = Y, method = \"Brent\", lower = 0.01, upper = 20)\noptim_result\n\n$par\n[1] 3.684667\n\n$value\n[1] 3367.684\n\n$counts\nfunction gradient \n      NA       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optimizer confirms that the MLE of \\(\\lambda\\) is approximately equal to the sample mean. This gives us confidence that our custom likelihood function and MLE implementation are correct.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Create squared age and dummy variables for region\nblueprinty &lt;- blueprinty %&gt;%\nmutate(age2 = age^2)\n\n# Construct the covariate matrix X (intercept, age, age^2, region dummies, iscustomer)\nX &lt;- model.matrix(~ age + age2 + region + iscustomer, data = blueprinty)\n\n# Outcome variable\nY &lt;- blueprinty$patents\n\n\n\n\n# Define log-likelihood function\npoisson_regression_loglik &lt;- function(beta, Y, X) {\nlambda &lt;- exp(X %*% beta)\n  loglik &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\nreturn(-loglik)  # negative log-likelihood for minimization\n}\n\n\n\n\n\n# Initial guess: all betas = 0\ninit_beta &lt;- rep(0, ncol(X))\n\n# Estimate MLE via optim()\nfit &lt;- optim(par = init_beta,\n            fn = poisson_regression_loglik,\n            Y = Y, X = X,\n            hessian = TRUE, method = \"BFGS\")\n\n# Extract coefficients\nbeta_hat &lt;- fit$par\n\n# Calculate standard errors from Hessian\nhessian &lt;- fit$hessian\nvcov_matrix &lt;- solve(hessian)\nse_beta &lt;- sqrt(diag(vcov_matrix))\n\n# Create table\ncoef_table &lt;- data.frame(\nCoefficient = beta_hat,\nStd_Error = se_beta,\nrow.names = colnames(X)\n)\n\nknitr::kable(coef_table, digits = 4, caption = \"MLE Coefficients and Standard Errors\")\n\n\nMLE Coefficients and Standard Errors\n\n\n\nCoefficient\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage2\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\n\n\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n            data = blueprinty, family = poisson())\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe coefficients from both optim() and glm() are nearly identical, indicating correct implementation.\nThe coefficient on iscustomer is positive and statistically significant, suggesting that firms using Blueprinty software tend to be granted more patents, controlling for other variables.\n\n\n\n\n# Counterfactual prediction: all firms as non-customers (iscustomer = 0)\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\n\n# Counterfactual prediction: all firms as customers (iscustomer = 1)\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n\n# Predicted patent counts under each case\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Average treatment effect\ntreatment_effect &lt;- mean(lambda_1 - lambda_0)\ntreatment_effect\n\n[1] 0.2178843\n\n\nOn average, being a Blueprinty customer increases the expected number of patents by about 0.22 over five years per firm."
  },
  {
    "objectID": "Blog/Project2/index.html#airbnb-case-study",
    "href": "Blog/Project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\nid = unique ID number for each unit\nlast_scraped = date when information scraped\nhost_since = date when host first listed the unit on Airbnb\ndays = last_scraped - host_since = number of days the unit has been listed\nroom_type = Entire home/apt., Private room, or Shared room\nbathrooms = number of bathrooms\nbedrooms = number of bedrooms\nprice = price per night (dollars)\nnumber_of_reviews = number of reviews for the unit on Airbnb\nreview_scores_cleanliness = a cleanliness score from reviews (1-10)\nreview_scores_location = a “quality of location” score from reviews (1-10)\nreview_scores_value = a “quality of value” score from reviews (1-10)\ninstant_bookable = “t” if instantly bookable, “f” if not\n\n\n\n\n\n\nExploratory Data Analysis\n\n# Load Airbnb data\nairbnb &lt;- read_csv(\"/home/jovyan/Downloads/SPRING/Marketing Analytics/MA_demo/mysite/Blog/Project2/airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# View structure\nglimpse(airbnb)\n\nRows: 40,628\nColumns: 14\n$ ...1                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;dbl&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;dbl&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;dbl&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;dbl&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;dbl&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FAL…\n\n# Check for missing data\ncolSums(is.na(airbnb))\n\n                     ...1                        id                      days \n                        0                         0                         0 \n             last_scraped                host_since                 room_type \n                        0                        35                         0 \n                bathrooms                  bedrooms                     price \n                      160                        76                         0 \n        number_of_reviews review_scores_cleanliness    review_scores_location \n                        0                     10195                     10254 \n      review_scores_value          instant_bookable \n                    10256                         0 \n\n\n\n\nData Cleaning\n\n# Remove rows with NA in key variables used in modeling\nairbnb_clean &lt;- airbnb %&gt;%\nfilter(!is.na(bathrooms),\n      !is.na(bedrooms),\n      !is.na(price),\n      !is.na(review_scores_cleanliness),\n      !is.na(review_scores_location),\n      !is.na(review_scores_value),\n      !is.na(number_of_reviews))\n\n# Convert categorical variables\nairbnb_clean &lt;- airbnb_clean %&gt;%\nmutate(\ninstant_bookable = ifelse(instant_bookable == \"t\", 1, 0),\nroom_type = as.factor(room_type),\nprice = as.numeric(price)\n)\n\n\n\nHistograms of Reviews and Price\n\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\ngeom_histogram(binwidth = 5, fill = \"steelblue\", alpha = 0.7) +\nlabs(title = \"Distribution of Number of Reviews\",\nx = \"Number of Reviews\", y = \"Count\") +\ntheme_minimal()\n\n\n\n\n\n\n\nggplot(airbnb_clean, aes(x = price)) +\ngeom_histogram(binwidth = 50, fill = \"darkorange\", alpha = 0.7) +\nlabs(title = \"Distribution of Price per Night\",\n      x = \"Price ($)\", y = \"Count\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model\nWe now estimate a Poisson model where the number of reviews is the dependent variable (as a proxy for bookings). Independent variables include: - price - bathrooms - bedrooms - review scores (cleanliness, location, value) - room type - instant bookable flag\n\n# Poisson regression model\npoisson_airbnb &lt;- glm(number_of_reviews ~ price + bathrooms + bedrooms +\n                        review_scores_cleanliness + review_scores_location +\n                        review_scores_value + room_type + instant_bookable,\n                  data = airbnb_clean, family = poisson())\n\nsummary(poisson_airbnb)\n\n\nCall:\nglm(formula = number_of_reviews ~ price + bathrooms + bedrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    room_type + instant_bookable, family = poisson(), data = airbnb_clean)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.714e+00  1.587e-02 233.976  &lt; 2e-16 ***\nprice                     -3.275e-05  8.521e-06  -3.843 0.000121 ***\nbathrooms                 -1.164e-01  3.786e-03 -30.751  &lt; 2e-16 ***\nbedrooms                   7.607e-02  2.001e-03  38.016  &lt; 2e-16 ***\nreview_scores_cleanliness  1.140e-01  1.486e-03  76.750  &lt; 2e-16 ***\nreview_scores_location    -8.065e-02  1.599e-03 -50.445  &lt; 2e-16 ***\nreview_scores_value       -9.749e-02  1.789e-03 -54.484  &lt; 2e-16 ***\nroom_typePrivate room      7.405e-03  2.734e-03   2.709 0.006747 ** \nroom_typeShared room      -2.262e-01  8.616e-03 -26.249  &lt; 2e-16 ***\ninstant_bookable                  NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 949198  on 30151  degrees of freedom\nAIC: 1070683\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Poisson model suggests that: - Higher review scores (especially cleanliness and value) are associated with more bookings - Instant bookable listings tend to get more reviews (suggesting convenience matters) - Room type strongly influences bookings: private rooms and shared rooms tend to get fewer reviews compared to entire homes - The effect of price is small and possibly negative, consistent with lower demand for higher-priced listings"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aashvi Trivedi",
    "section": "",
    "text": "Here are a few details about me!"
  },
  {
    "objectID": "HW2/hw2_questions.html",
    "href": "HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "HW2/hw2_questions.html#blueprinty-case-study",
    "href": "HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "HW2/hw2_questions.html#airbnb-case-study",
    "href": "HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "Blog/Project1/index.html",
    "href": "Blog/Project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "Blog/Project1/index.html#introduction",
    "href": "Blog/Project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "Blog/Project1/index.html#data",
    "href": "Blog/Project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n# Load necessary packages\nlibrary(haven)    # to read .dta\nlibrary(dplyr)    # for data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)  # for plots\nlibrary(broom)    # for tidy regression output\nlibrary(knitr)    # for tables\n\n# Read the data\ndf &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Show the first few rows\nhead(df)\n\n# A tibble: 6 × 51\n  treatment control ratio     ratio2 ratio3 size    size25 size50 size100 sizeno\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1         0       1 0 [Contr…      0      0 0 [Con…      0      0       0      0\n2         0       1 0 [Contr…      0      0 0 [Con…      0      0       0      0\n3         1       0 1              0      0 3 [$10…      0      0       1      0\n4         1       0 1              0      0 4 [Uns…      0      0       0      1\n5         1       0 1              0      0 2 [$50…      0      1       0      0\n6         0       1 0 [Contr…      0      0 0 [Con…      0      0       0      0\n# ℹ 41 more variables: ask &lt;dbl+lbl&gt;, askd1 &lt;dbl&gt;, askd2 &lt;dbl&gt;, askd3 &lt;dbl&gt;,\n#   ask1 &lt;dbl&gt;, ask2 &lt;dbl&gt;, ask3 &lt;dbl&gt;, amount &lt;dbl&gt;, gave &lt;dbl&gt;,\n#   amountchange &lt;dbl&gt;, hpa &lt;dbl&gt;, ltmedmra &lt;dbl&gt;, freq &lt;dbl&gt;, years &lt;dbl&gt;,\n#   year5 &lt;dbl&gt;, mrm2 &lt;dbl&gt;, dormant &lt;dbl&gt;, female &lt;dbl&gt;, couple &lt;dbl&gt;,\n#   state50one &lt;dbl&gt;, nonlit &lt;dbl&gt;, cases &lt;dbl&gt;, statecnt &lt;dbl&gt;,\n#   stateresponse &lt;dbl&gt;, stateresponset &lt;dbl&gt;, stateresponsec &lt;dbl&gt;,\n#   stateresponsetminc &lt;dbl&gt;, perbush &lt;dbl&gt;, close25 &lt;dbl&gt;, red0 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# T-test: compare means of months since last donation\nt_test_mrm2 &lt;- t.test(mrm2 ~ treatment, data = df)\nt_test_mrm2\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\n\nThe average number of months since last donation is not statistically significantly different between treatment and control groups (p-value &gt; 0.05). This suggests randomization worked well and groups were balanced — supporting the integrity of the experiment.\n\n# Linear regression: mrm2 on treatment\nlm_mrm2 &lt;- lm(mrm2 ~ treatment, data = df)\nsummary(lm_mrm2)\n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\n\nAs expected, the regression confirms the t-test: no significant effect of treatment on months since last donation. Coefficient is small and p-value is large. Again, this supports random assignment."
  },
  {
    "objectID": "Blog/Project1/index.html#experimental-results",
    "href": "Blog/Project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nlibrary(ggplot2)\n\n# Calculate proportion of donations by group\ndf %&gt;%\ngroup_by(treatment) %&gt;%\nsummarise(gave_rate = mean(gave)) %&gt;%\nggplot(aes(x = factor(treatment, labels = c(\"Control\", \"Treatment\")),\n            y = gave_rate,\n            fill = factor(treatment))) +\ngeom_col() +\nlabs(title = \"Proportion of People Who Donated\",\n    x = \"Group\",\n    y = \"Donation Rate\") +\ntheme_minimal() +\ntheme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# T-test for donation outcome\nt.test(gave ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n\n\n# Linear regression for same effect\nlm_gave &lt;- lm(gave ~ treatment, data = df)\nsummary(lm_gave)\n\n\nCall:\nlm(formula = gave ~ treatment, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\nThe bar plot and t-test show that the treatment group had a higher donation rate than the control group. The difference is statistically significant (p-value &lt; 0.05), meaning the matching offer increased people’s likelihood to donate. This supports the paper’s claim that matching gifts raise response rates.\n\n# Probit regression\nprobit_model &lt;- glm(gave ~ treatment, data = df, family = binomial(link = \"probit\"))\nsummary(probit_model)\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe probit regression confirms the earlier findings: being assigned to the treatment increases the probability of donating. This aligns with Table 3, Column 1 in the original paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Subset to treatment group only\ntreat_df &lt;- df %&gt;% filter(treatment == 1)\n\n# Create dummy variable for 1:1 match (if neither 2:1 nor 3:1)\ntreat_df &lt;- treat_df %&gt;%\nmutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n\n# 1:1 vs 2:1\nt_1v2 &lt;- t.test(gave ~ ratio2, data = treat_df %&gt;% filter(ratio1 == 1 | ratio2 == 1))\nt_1v2\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio2\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 0 mean in group 1 \n     0.02074912      0.02263338 \n\n# 2:1 vs 3:1\nt_2v3 &lt;- t.test(gave ~ ratio3, data = treat_df %&gt;% filter(ratio2 == 1 | ratio3 == 1))\nt_2v3\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio3\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 0 mean in group 1 \n     0.02263338      0.02273340 \n\n\nThe t-tests reveal no statistically significant differences in response rates between 1:1 and 2:1 match groups or 2:1 and 3:1 match groups. This supports the authors’ statement that larger match ratios do not lead to higher likelihood of giving, once any match is offered.\n\n# Regress giving on ratio dummies\nlm_ratios &lt;- lm(gave ~ ratio1 + ratio2 + ratio3, data = treat_df)\nsummary(lm_ratios)\n\n\nCall:\nlm(formula = gave ~ ratio1 + ratio2 + ratio3, data = treat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.022733   0.001392  16.335   &lt;2e-16 ***\nratio1      -0.001984   0.001968  -1.008    0.313    \nratio2      -0.000100   0.001968  -0.051    0.959    \nratio3             NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\nIn the regression, all match ratio dummy variables are statistically insignificant. This confirms that the generosity of the match ratio does not meaningfully affect donation rates, and the mere presence of a match drives the behavioral change.\n\n# Raw donation rates by ratio group\ntreat_df %&gt;%\ngroup_by(ratio) %&gt;%\nsummarise(response_rate = mean(gave))\n\n# A tibble: 3 × 2\n  ratio     response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1 1                0.0207\n2 2                0.0226\n3 3                0.0227\n\n\nThe raw response rates are also similar across 1:1, 2:1, and 3:1 groups — further reinforcing the conclusion that larger matches don’t help more than a basic 1:1 match.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# T-test: effect of treatment on donation amount (unconditional)\nt.test(amount ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = -1.9183, df = 36216, p-value = 0.05509\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.310555423  0.003344493\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8132678       0.9668733 \n\n\n\n# Linear regression: amount ~ treatment\nlm_amt_all &lt;- lm(amount ~ treatment, data = df)\nsummary(lm_amt_all)\n\n\nCall:\nlm(formula = amount ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\nThe average donation amount is slightly higher in the treatment group, but the difference is not statistically significant. This suggests that while more people gave, the average amount given (including non-donors) was not substantially larger.\n\n# Filter to only people who donated (amount &gt; 0)\ndf_donors &lt;- df %&gt;% filter(amount &gt; 0)\n\n# T-test: among donors only\nt.test(amount ~ treatment, data = df_donors)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = 0.58461, df = 557.46, p-value = 0.559\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -3.937240  7.274027\nsample estimates:\nmean in group 0 mean in group 1 \n       45.54027        43.87188 \n\n\n\n# Linear regression: amount ~ treatment (among donors)\nlm_amt_donors &lt;- lm(amount ~ treatment, data = df_donors)\nsummary(lm_amt_donors)\n\n\nCall:\nlm(formula = amount ~ treatment, data = df_donors)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\nAmong those who donated, the treatment group does not give significantly more than the control group. The treatment coefficient does not have a strong causal interpretation here, as it could reflect selection (different kinds of people chose to give in treatment vs. control).\n\nlibrary(ggplot2)\n\n# Add group labels\ndf_donors &lt;- df_donors %&gt;%\nmutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\n# Plot for Control group\ndf_donors %&gt;% \nfilter(group == \"Control\") %&gt;%\nggplot(aes(x = amount)) +\ngeom_histogram(binwidth = 5, fill = \"steelblue\", alpha = 0.7) +\ngeom_vline(aes(xintercept = mean(amount)), color = \"red\", linetype = \"dashed\") +\nlabs(title = \"Control Group: Donation Amounts\",\n    x = \"Amount Donated\", y = \"Count\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n# Plot for Treatment group\ndf_donors %&gt;% \nfilter(group == \"Treatment\") %&gt;%\nggplot(aes(x = amount)) +\ngeom_histogram(binwidth = 5, fill = \"seagreen\", alpha = 0.7) +\ngeom_vline(aes(xintercept = mean(amount)), color = \"red\", linetype = \"dashed\") +\nlabs(title = \"Treatment Group: Donation Amounts\",\n    x = \"Amount Donated\", y = \"Count\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\nBoth groups show right-skewed distributions of donation amounts. The dashed red lines represent the average amount given. There is no dramatic difference in the average donation size between treatment and control."
  },
  {
    "objectID": "Blog/Project1/index.html#simulation-experiment",
    "href": "Blog/Project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate 10,000 draws from Bernoulli p=0.022 (treatment)\ntreatment_sim &lt;- rbinom(10000, 1, 0.022)\n\n# Simulate 10,000 draws from Bernoulli p=0.018 (control)\ncontrol_sim &lt;- rbinom(10000, 1, 0.018)\n\n# Compute difference at each point\ndiffs &lt;- treatment_sim - control_sim\n\n# Cumulative average of the differences\ncum_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Plot\nggplot(data.frame(index = 1:10000, cum_avg = cum_avg), aes(x = index, y = cum_avg)) +\ngeom_line(color = \"steelblue\") +\ngeom_hline(yintercept = 0.004, linetype = \"dashed\", color = \"red\") +\nlabs(title = \"Law of Large Numbers Simulation\",\n    x = \"Number of Observations\",\n    y = \"Cumulative Average of Differences\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows the cumulative average of differences in donation probabilities converging toward 0.004 — the true difference (2.2% - 1.8%). This illustrates the Law of Large Numbers: as sample size grows, sample statistics converge to their true values.\n\n\nCentral Limit Theorem\n\nsimulate_diffs &lt;- function(n, reps = 1000, p_control = 0.018, p_treatment = 0.022) {\nreplicate(reps, {\n    control &lt;- rbinom(n, 1, p_control)\n    treatment &lt;- rbinom(n, 1, p_treatment)\n    mean(treatment) - mean(control)\n})\n}\n\nsizes &lt;- c(50, 200, 500, 1000)\ndiffs_by_size &lt;- lapply(sizes, simulate_diffs)\n\n# Plot histograms\npar(mfrow = c(2, 2))  # 2 rows x 2 columns layout\n\nfor (i in 1:4) {\nhist(diffs_by_size[[i]], breaks = 30, main = paste(\"Sample size =\", sizes[i]),\n    xlab = \"Mean difference (treatment - control)\", col = \"lightblue\", border = \"white\")\nabline(v = 0, col = \"red\", lwd = 2, lty = 2)\n}\n\n\n\n\n\n\n\n\nAs sample size increases, the distribution of average differences becomes more concentrated around the true mean and more symmetric. With larger samples (like 1000), the distribution approximates a normal curve — demonstrating the Central Limit Theorem."
  },
  {
    "objectID": "HW1/hw1_questions.html",
    "href": "HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/hw1_questions.html#introduction",
    "href": "HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/hw1_questions.html#data",
    "href": "HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "HW1/hw1_questions.html#experimental-results",
    "href": "HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "HW1/hw1_questions.html#simulation-experiment",
    "href": "HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "Blog/Project3/index.html",
    "href": "Blog/Project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "Blog/Project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "Blog/Project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "Blog/Project3/index.html#simulate-conjoint-data",
    "href": "Blog/Project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n\n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "Blog/Project3/index.html#preparing-the-data-for-estimation",
    "href": "Blog/Project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# Load required tidyverse libraries\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load the conjoint data\nconjoint &lt;- read_csv(\"conjoint_data.csv\")\n\nRows: 3000 Columns: 6\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): brand, ad\ndbl (4): resp, task, choice, price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View structure\nglimpse(conjoint)\n\nRows: 3,000\nColumns: 6\n$ resp   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ task   &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, …\n$ choice &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, …\n$ brand  &lt;chr&gt; \"N\", \"H\", \"P\", \"N\", \"P\", \"N\", \"P\", \"H\", \"N\", \"P\", \"P\", \"P\", \"N\"…\n$ ad     &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"N…\n$ price  &lt;dbl&gt; 28, 16, 16, 32, 16, 24, 8, 24, 24, 28, 12, 24, 20, 28, 16, 12, …\n\n\n\n\nConvert categorical variables into binary (dummy) format\n\n# Recode brand and ad into dummy variables\nconjoint_prep &lt;- conjoint %&gt;%\nmutate(\n    brand_Netflix = ifelse(brand == \"N\", 1, 0),\n    brand_Prime   = ifelse(brand == \"P\", 1, 0),\n    # Hulu is baseline (reference)\n    ad_yes = ifelse(ad == \"Yes\", 1, 0)  # Yes = 1, No = 0 (No is baseline)\n)\n\n\n\n\nCreate Design Matrix (X) and Response Vector (y)\n\n# Sort data just in case\nconjoint_prep &lt;- conjoint_prep %&gt;%\narrange(resp, task)\n\n# Create matrix of covariates (no intercept)\nX &lt;- conjoint_prep %&gt;%\nselect(brand_Netflix, brand_Prime, ad_yes, price) %&gt;%\nas.matrix()\n\n# Create choice vector\ny &lt;- conjoint_prep$choice\n\n\n\n\nOutput Check\n\ndim(X)         # Should be 100 * 10 * 3 rows, and 4 columns\n\n[1] 3000    4\n\ntable(y)       # Should have mostly 0s and one 1 per task\n\ny\n   0    1 \n2000 1000 \n\nhead(X)\n\n     brand_Netflix brand_Prime ad_yes price\n[1,]             1           0      1    28\n[2,]             0           0      1    16\n[3,]             0           1      1    16\n[4,]             1           0      1    32\n[5,]             0           1      1    16\n[6,]             1           0      1    24\n\n\n\n\n\nInterpretation\n\nEach row of the matrix X now corresponds to one alternative from one task. We include dummy variables for brand (Netflix, Prime; Hulu is the base), a dummy for ad presence, and a numeric price variable. The response vector y contains a 1 for the chosen alternative and 0s for the others within each choice task."
  },
  {
    "objectID": "Blog/Project3/index.html#estimation-via-maximum-likelihood",
    "href": "Blog/Project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n# MNL Log-Likelihood Function\nmnl_loglik &lt;- function(beta, X, y, n_alts = 3) {\nn &lt;- length(y)\nutilities &lt;- X %*% beta\nutilities_mat &lt;- matrix(utilities, ncol = n_alts, byrow = TRUE)\nexp_util &lt;- exp(utilities_mat)\nprobs &lt;- exp_util / rowSums(exp_util)\n\n# Reshape y to be matrix matching the structure\ny_mat &lt;- matrix(y, ncol = n_alts, byrow = TRUE)\nlog_probs &lt;- log(probs[y_mat == 1])\n\n# Return negative log-likelihood for minimization\nreturn(-sum(log_probs))\n}\n\n\n\nEstimate Parameters Using optim()\n\n# Initial beta guesses\ninit_beta &lt;- rep(0, ncol(X))  # 4 coefficients\n\n# Optimize log-likelihood\nmle_result &lt;- optim(par = init_beta,\n                    fn = mnl_loglik,\n                    X = X, y = y,\n                    method = \"BFGS\",\n                    hessian = TRUE)\n\n# Extract coefficients\nbeta_hat &lt;- mle_result$par\nhess &lt;- mle_result$hessian\nse_hat &lt;- sqrt(diag(solve(hess)))\n\n\n\n\nConstruct Coefficient Table\n\n# 95% confidence intervals\nz_val &lt;- qnorm(0.975)\nlower_ci &lt;- beta_hat - z_val * se_hat\nupper_ci &lt;- beta_hat + z_val * se_hat\n\n# Create results table\nresults &lt;- data.frame(\nCoefficient = c(\"Netflix\", \"Prime\", \"Ads\", \"Price\"),\nEstimate = round(beta_hat, 4),\nStd_Error = round(se_hat, 4),\nCI_Lower = round(lower_ci, 4),\nCI_Upper = round(upper_ci, 4)\n)\n\n# Display as a table\nknitr::kable(results, caption = \"MLE Estimates for MNL Model\")\n\n\nMLE Estimates for MNL Model\n\n\nCoefficient\nEstimate\nStd_Error\nCI_Lower\nCI_Upper\n\n\n\n\nNetflix\n0.9412\n0.1110\n0.7236\n1.1588\n\n\nPrime\n0.5016\n0.1111\n0.2839\n0.7194\n\n\nAds\n-0.7320\n0.0878\n-0.9041\n-0.5599\n\n\nPrice\n-0.0995\n0.0063\n-0.1119\n-0.0871\n\n\n\n\n\n\n\n\nInterpretation\n\nAll four estimated coefficients are interpretable as part-worth utilities:\n\nPositive estimates for Netflix and Prime reflect a preference over the baseline brand (Hulu).\nA negative coefficient for Ads means people strongly dislike ads.\nA negative coefficient for Price means higher prices reduce the probability of being chosen, as expected.\n\nConfidence intervals confirm that these effects are statistically significant at the 95% level.\n\n\n\n\n5. Estimation via Bayesian Methods\n\n# Log-prior: N(0, 5^2) for binary attributes, N(0, 1^2) for price\nlog_prior &lt;- function(beta) {\n# Assume: beta = c(beta_Netflix, beta_Prime, beta_Ads, beta_Price)\nprior1 &lt;- dnorm(beta[1], mean = 0, sd = 5, log = TRUE)\nprior2 &lt;- dnorm(beta[2], mean = 0, sd = 5, log = TRUE)\nprior3 &lt;- dnorm(beta[3], mean = 0, sd = 5, log = TRUE)\nprior4 &lt;- dnorm(beta[4], mean = 0, sd = 1, log = TRUE)\nreturn(prior1 + prior2 + prior3 + prior4)\n}\n\n# Log-posterior = log-likelihood + log-prior\nlog_posterior &lt;- function(beta, X, y) {\n-mnl_loglik(beta, X, y) + log_prior(beta)\n}\n\n\n\n\nImplement the Metropolis-Hastings Sampler\n\n# Initialize\nn_iter &lt;- 11000\nn_params &lt;- ncol(X)\nbeta_draws &lt;- matrix(NA, nrow = n_iter, ncol = n_params)\nbeta_current &lt;- rep(0, n_params)\n\n# Proposal standard deviations\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\n\n# Log-posterior at starting value\nlog_post_current &lt;- log_posterior(beta_current, X, y)\n\n# Run MH sampler\nset.seed(42)\nfor (i in 1:n_iter) {\n# Propose new beta\nbeta_proposed &lt;- beta_current + rnorm(n_params, mean = 0, sd = proposal_sd)\nlog_post_proposed &lt;- log_posterior(beta_proposed, X, y)\n\n# Acceptance probability\naccept_prob &lt;- exp(log_post_proposed - log_post_current)\n\nif (runif(1) &lt; accept_prob) {\n    beta_current &lt;- beta_proposed\n    log_post_current &lt;- log_post_proposed\n}\n\nbeta_draws[i, ] &lt;- beta_current\n}\n\n# Remove burn-in\nposterior &lt;- beta_draws[1001:n_iter, ]\ncolnames(posterior) &lt;- c(\"Netflix\", \"Prime\", \"Ads\", \"Price\")\n\n\n\n\nPosterior Summary Table\n\n# Posterior stats\npost_mean &lt;- apply(posterior, 2, mean)\npost_sd &lt;- apply(posterior, 2, sd)\nci_lower &lt;- apply(posterior, 2, quantile, 0.025)\nci_upper &lt;- apply(posterior, 2, quantile, 0.975)\n\nbayes_results &lt;- data.frame(\nParameter = colnames(posterior),\nMean = round(post_mean, 4),\nSD = round(post_sd, 4),\nCI_Lower = round(ci_lower, 4),\nCI_Upper = round(ci_upper, 4)\n)\n\nknitr::kable(bayes_results, caption = \"Posterior Estimates from Bayesian MNL\")\n\n\nPosterior Estimates from Bayesian MNL\n\n\n\nParameter\nMean\nSD\nCI_Lower\nCI_Upper\n\n\n\n\nNetflix\nNetflix\n0.9475\n0.1138\n0.7308\n1.1659\n\n\nPrime\nPrime\n0.4974\n0.1163\n0.2759\n0.7317\n\n\nAds\nAds\n-0.7369\n0.0884\n-0.9016\n-0.5638\n\n\nPrice\nPrice\n-0.1000\n0.0063\n-0.1123\n-0.0880\n\n\n\n\n\n\n\n\nTrace Plot and Posterior Histogram\n\n# Trace plot for Price coefficient\nplot(posterior[, \"Price\"], type = \"l\", col = \"darkgreen\",\n    main = \"Trace Plot for Price Coefficient\",\n    xlab = \"Iteration\", ylab = \"Beta (Price)\")\n\n\n\n\n\n\n\n# Posterior histogram\nhist(posterior[, \"Price\"], breaks = 40, col = \"lightblue\",\n    main = \"Posterior Distribution of Price Coefficient\",\n    xlab = \"Beta (Price)\")\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nThe posterior means are close to the MLE estimates, which provides reassurance that both estimation approaches converge to similar results.\n- Bayesian posterior standard deviations serve a similar purpose as standard errors.\n- The 95% credible intervals from the Bayesian method are slightly wider than MLE confidence intervals, reflecting uncertainty from the prior.\n- The trace plot shows good mixing, and the posterior distribution of beta_Price is unimodal and centered near the MLE value."
  },
  {
    "objectID": "Blog/Project3/index.html#discussion",
    "href": "Blog/Project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpretation of Parameter Estimates\nIf we didn’t know the data was simulated, we would still observe: - \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime} &gt; 0\\): This suggests that, on average, respondents prefer Netflix the most, followed by Amazon Prime, relative to the baseline (Hulu). - \\(\\beta_\\text{price} &lt; 0\\): This negative coefficient makes intuitive sense — as price increases, the likelihood of choosing that alternative decreases. - \\(\\beta_\\text{ads} &lt; 0\\): Respondents dislike advertisements, which aligns with consumer expectations.\nThe fact that \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) indicates a stronger preference for Netflix relative to Prime (and much more than Hulu), holding other attributes constant. These rankings reflect relative part-worth utilities for brand, ads, and price, which are fundamental to interpreting MNL model output.\n\n\nMoving to a Multi-Level (Hierarchical) Model\nIn a hierarchical or random-parameter logit model, we allow each individual to have their own vector of preference weights \\(\\beta_i\\). These individual-level betas are assumed to be drawn from a population distribution, such as:\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nTo simulate data from such a model: - Instead of using one fixed \\(\\beta\\) for all respondents, we would first draw a unique \\(\\beta_i\\) for each individual from a normal distribution with a shared population mean and covariance. - Then we would simulate each respondent’s choices using their own \\(\\beta_i\\).\nTo estimate this model: - We would use Hierarchical Bayes (HB) or Simulated Maximum Likelihood to recover both the population-level parameters (mean and covariance of \\(\\beta_i\\)) and the individual-level betas. - This requires either Gibbs sampling with data augmentation or advanced MCMC methods like Metropolis-within-Gibbs.\nThese models are more flexible and better reflect heterogeneity in preferences — which is essential when working with real-world conjoint data where different consumers respond differently to the same product features."
  },
  {
    "objectID": "HW3/hw3_questions.html",
    "href": "HW3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "HW3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "HW3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "HW3/hw3_questions.html#simulate-conjoint-data",
    "href": "HW3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "HW3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "HW3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data"
  },
  {
    "objectID": "HW3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "HW3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "HW3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "HW3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "HW3/hw3_questions.html#discussion",
    "href": "HW3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  },
  {
    "objectID": "Blog/Project4/index.html",
    "href": "Blog/Project4/index.html",
    "title": "From Clusters to Classifications: K-Means and KNN in R",
    "section": "",
    "text": "# Install & Load Required Packages\ninstall.packages(\"cluster\")\n\nInstalling package into '/home/jovyan/.rsm-msba/R/4.4.1'\n(as 'lib' is unspecified)\n\ninstall.packages(\"factoextra\")\n\nInstalling package into '/home/jovyan/.rsm-msba/R/4.4.1'\n(as 'lib' is unspecified)\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/jovyan/.rsm-msba/R/4.4.1'\n(as 'lib' is unspecified)\n\nlibrary(cluster)\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(ggplot2)\n\n# Load and Preprocess Data\n# Update path if needed:\npenguins &lt;- read.csv(\"/home/jovyan/Downloads/SPRING/Marketing Analytics/MA_demo/mysite/Blog/Project4/palmer_penguins.csv\")\n\n# Keep only relevant columns\npenguins &lt;- na.omit(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n# Normalize the data\npenguins_scaled &lt;- scale(penguins)\n\n# Define Manual K-Means Function\neuclidean_dist &lt;- function(a, b) sqrt(sum((a - b)^2))\n\nkmeans_manual &lt;- function(data, k, max_iter = 100) {\nset.seed(123)\ncenters &lt;- data[sample(1:nrow(data), k), ]\nfor (i in 1:max_iter) {\n    clusters &lt;- apply(data, 1, function(x) {\n    which.min(apply(centers, 1, function(y) euclidean_dist(x, y)))\n    })\n    new_centers &lt;- sapply(1:k, function(j) colMeans(data[clusters == j, , drop = FALSE]))\n    new_centers &lt;- t(new_centers)\n    if (all(abs(new_centers - centers) &lt; 1e-6)) break\n    centers &lt;- new_centers\n}\nreturn(list(clusters = clusters, centers = centers))\n}\n\n# Apply Manual K-Means for K = 3\nresult &lt;- kmeans_manual(penguins_scaled, k = 3)\npenguins$manual_cluster &lt;- as.factor(result$clusters)\n\n# Plot Manual K-Means Clusters\nggplot(penguins, aes(bill_length_mm, flipper_length_mm, color = manual_cluster)) +\ngeom_point(size = 2, alpha = 0.8) +\nggtitle(\"K-Means Clustering (Manual)\") +\ntheme_minimal()\n\n\n\n\n\n\n\n# Compare with Built-in kmeans()\nk_builtin &lt;- kmeans(penguins_scaled, centers = 3)\npenguins$builtin_cluster &lt;- as.factor(k_builtin$cluster)\n\nggplot(penguins, aes(bill_length_mm, flipper_length_mm, color = builtin_cluster)) +\ngeom_point(size = 2, alpha = 0.8) +\nggtitle(\"K-Means Clustering (Built-in Function)\") +\ntheme_minimal()\n\n\n\n\n\n\n\n# Evaluate Optimal K using WSS & Silhouette\nfviz_nbclust(penguins_scaled, kmeans, method = \"wss\") +\nggtitle(\"Elbow Method for Optimal K\")\n\n\n\n\n\n\n\nfviz_nbclust(penguins_scaled, kmeans, method = \"silhouette\") +\nggtitle(\"Silhouette Score for Optimal K\")\n\n\n\n\n\n\n\n# Bonus: Calculate Silhouette Scores for K = 2 to 7\n\nsil_scores &lt;- numeric()\nwss_values &lt;- numeric()\n\nfor (k in 2:7) {\nkm &lt;- kmeans(penguins_scaled, centers = k)\nss &lt;- silhouette(km$cluster, dist(penguins_scaled))\nsil_scores[k] &lt;- mean(ss[, 3])\nwss_values[k] &lt;- km$tot.withinss\n}\n\n# Combine into data frame\nk_values &lt;- 2:7\neval_df &lt;- data.frame(\nK = k_values,\nSilhouette = sil_scores[k_values],\nWSS = wss_values[k_values]\n)\n\n# Plot custom silhouette and WSS results\nggplot(eval_df, aes(x = K)) +\ngeom_line(aes(y = Silhouette), color = \"blue\") +\ngeom_point(aes(y = Silhouette), color = \"blue\") +\ngeom_line(aes(y = (WSS - min(WSS)) / (max(WSS) - min(WSS))), color = \"red\") +\ngeom_point(aes(y = (WSS - min(WSS)) / (max(WSS) - min(WSS))), color = \"red\") +\nlabs(title = \"Silhouette (Blue) vs Normalized WSS (Red)\", y = \"Score (scaled)\", x = \"Number of Clusters (K)\") +\ntheme_minimal()"
  },
  {
    "objectID": "Blog/Project4/index.html#k-means",
    "href": "Blog/Project4/index.html#k-means",
    "title": "From Clusters to Classifications: K-Means and KNN in R",
    "section": "",
    "text": "# Install & Load Required Packages\ninstall.packages(\"cluster\")\n\nInstalling package into '/home/jovyan/.rsm-msba/R/4.4.1'\n(as 'lib' is unspecified)\n\ninstall.packages(\"factoextra\")\n\nInstalling package into '/home/jovyan/.rsm-msba/R/4.4.1'\n(as 'lib' is unspecified)\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/jovyan/.rsm-msba/R/4.4.1'\n(as 'lib' is unspecified)\n\nlibrary(cluster)\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(ggplot2)\n\n# Load and Preprocess Data\n# Update path if needed:\npenguins &lt;- read.csv(\"/home/jovyan/Downloads/SPRING/Marketing Analytics/MA_demo/mysite/Blog/Project4/palmer_penguins.csv\")\n\n# Keep only relevant columns\npenguins &lt;- na.omit(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n# Normalize the data\npenguins_scaled &lt;- scale(penguins)\n\n# Define Manual K-Means Function\neuclidean_dist &lt;- function(a, b) sqrt(sum((a - b)^2))\n\nkmeans_manual &lt;- function(data, k, max_iter = 100) {\nset.seed(123)\ncenters &lt;- data[sample(1:nrow(data), k), ]\nfor (i in 1:max_iter) {\n    clusters &lt;- apply(data, 1, function(x) {\n    which.min(apply(centers, 1, function(y) euclidean_dist(x, y)))\n    })\n    new_centers &lt;- sapply(1:k, function(j) colMeans(data[clusters == j, , drop = FALSE]))\n    new_centers &lt;- t(new_centers)\n    if (all(abs(new_centers - centers) &lt; 1e-6)) break\n    centers &lt;- new_centers\n}\nreturn(list(clusters = clusters, centers = centers))\n}\n\n# Apply Manual K-Means for K = 3\nresult &lt;- kmeans_manual(penguins_scaled, k = 3)\npenguins$manual_cluster &lt;- as.factor(result$clusters)\n\n# Plot Manual K-Means Clusters\nggplot(penguins, aes(bill_length_mm, flipper_length_mm, color = manual_cluster)) +\ngeom_point(size = 2, alpha = 0.8) +\nggtitle(\"K-Means Clustering (Manual)\") +\ntheme_minimal()\n\n\n\n\n\n\n\n# Compare with Built-in kmeans()\nk_builtin &lt;- kmeans(penguins_scaled, centers = 3)\npenguins$builtin_cluster &lt;- as.factor(k_builtin$cluster)\n\nggplot(penguins, aes(bill_length_mm, flipper_length_mm, color = builtin_cluster)) +\ngeom_point(size = 2, alpha = 0.8) +\nggtitle(\"K-Means Clustering (Built-in Function)\") +\ntheme_minimal()\n\n\n\n\n\n\n\n# Evaluate Optimal K using WSS & Silhouette\nfviz_nbclust(penguins_scaled, kmeans, method = \"wss\") +\nggtitle(\"Elbow Method for Optimal K\")\n\n\n\n\n\n\n\nfviz_nbclust(penguins_scaled, kmeans, method = \"silhouette\") +\nggtitle(\"Silhouette Score for Optimal K\")\n\n\n\n\n\n\n\n# Bonus: Calculate Silhouette Scores for K = 2 to 7\n\nsil_scores &lt;- numeric()\nwss_values &lt;- numeric()\n\nfor (k in 2:7) {\nkm &lt;- kmeans(penguins_scaled, centers = k)\nss &lt;- silhouette(km$cluster, dist(penguins_scaled))\nsil_scores[k] &lt;- mean(ss[, 3])\nwss_values[k] &lt;- km$tot.withinss\n}\n\n# Combine into data frame\nk_values &lt;- 2:7\neval_df &lt;- data.frame(\nK = k_values,\nSilhouette = sil_scores[k_values],\nWSS = wss_values[k_values]\n)\n\n# Plot custom silhouette and WSS results\nggplot(eval_df, aes(x = K)) +\ngeom_line(aes(y = Silhouette), color = \"blue\") +\ngeom_point(aes(y = Silhouette), color = \"blue\") +\ngeom_line(aes(y = (WSS - min(WSS)) / (max(WSS) - min(WSS))), color = \"red\") +\ngeom_point(aes(y = (WSS - min(WSS)) / (max(WSS) - min(WSS))), color = \"red\") +\nlabs(title = \"Silhouette (Blue) vs Normalized WSS (Red)\", y = \"Score (scaled)\", x = \"Number of Clusters (K)\") +\ntheme_minimal()"
  },
  {
    "objectID": "Blog/Project4/index.html#k-nearest-neighbors",
    "href": "Blog/Project4/index.html#k-nearest-neighbors",
    "title": "From Clusters to Classifications: K-Means and KNN in R",
    "section": "2. K Nearest Neighbors",
    "text": "2. K Nearest Neighbors\n\n# Generate Synthetic Training Data\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nboundary &lt;- sin(4 * x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ntrain &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n# Generate Synthetic Test Data\nset.seed(99)\nx1_test &lt;- runif(n, -3, 3)\nx2_test &lt;- runif(n, -3, 3)\nboundary_test &lt;- sin(4 * x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ntest &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n# Plot the Training Data\nlibrary(ggplot2)\nggplot(train, aes(x = x1, y = x2, color = y)) +\ngeom_point(size = 2) +\n  stat_function(fun = function(x) sin(4 * x) + x, color = \"black\", linetype = \"dashed\") +\nggtitle(\"Synthetic KNN Training Data with Wiggly Boundary\") +\ntheme_minimal()\n\n\n\n\n\n\n\n# Implement KNN Manually\neuclidean_distance &lt;- function(a, b) {\nsqrt(sum((a - b)^2))\n}\n\nknn_predict &lt;- function(train, test, k) {\npred &lt;- vector(\"character\", nrow(test))\nfor (i in 1:nrow(test)) {\n    dists &lt;- apply(train[, c(\"x1\", \"x2\")], 1, function(row) euclidean_distance(row, test[i, c(\"x1\", \"x2\")]))\n    neighbors &lt;- train[order(dists), ][1:k, ]\n    pred[i] &lt;- names(which.max(table(neighbors$y)))\n}\nreturn(as.factor(pred))\n}\n\n\n# Evaluate Accuracy for k = 1 to 30\naccuracy &lt;- numeric(30)\nfor (k in 1:30) {\npred &lt;- knn_predict(train, test, k)\naccuracy[k] &lt;- mean(pred == test$y)\n}\n\n# Plot Accuracy vs K\naccuracy_df &lt;- data.frame(k = 1:30, accuracy = accuracy)\n\nggplot(accuracy_df, aes(x = k, y = accuracy)) +\ngeom_line(color = \"steelblue\") +\ngeom_point(color = \"darkred\") +\nlabs(title = \"KNN Accuracy vs. K\",\n    x = \"Number of Neighbors (K)\",\n    y = \"Accuracy on Test Set\") +\ntheme_minimal()"
  },
  {
    "objectID": "HW4/hw4_questions.html",
    "href": "HW4/hw4_questions.html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "HW4/hw4_questions.html#a.-k-means",
    "href": "HW4/hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "HW4/hw4_questions.html#b.-latent-class-mnl",
    "href": "HW4/hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "HW4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "HW4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "HW4/hw4_questions.html#b.-key-drivers-analysis",
    "href": "HW4/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]